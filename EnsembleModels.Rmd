---
title: "EnsembleModels"
author: "Lisa"
date: "March 12, 2015"
output: pdf_document
---

# Introduction

I applied a package **caretEnsemble** for making ensembles of caret models. It written by *Zach Mayer* and his gitbut website is <https://github.com/zachmayer/caretEnsemble>. The **caretEnsemble** includes 2 different algorithms for combining models:

1. Greedy stepwise ensembles (returns a weight for each model), using **caretEnsemble**.
2. Stacks of caret models, using **caretStack**. The stacking algorithm simply builds a second caret model on top of the existing models (using their predictions as input).



###Resourse:

* Binary classification model with caretEnsemble on gist of *Zach Mayer* <https://gist.github.com/zachmayer/5179418/>
* A brief introduction to caretEnsemble <http://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html>


```{r setup,include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```


##Load libraries

```{r,echo=FALSE,message=FALSE}

#Setup
rm(list = ls(all = TRUE)) 
gc(reset=TRUE) #maximum space for using
 
#Libraries
library(devtools)
#install_github('caretEnsemble', 'zachmayer')  #Install zach's caretEnsemble package
library(caretEnsemble)
#devtools::install_github('topepo/caret/pkg/caret') #Install the development version from GitHub
library(caret)
library ( kernlab ) 
library(gbm)
library(ggplot2)
library(doParallel)
library(plyr)
library(dplyr)

```

I deleted missing values (*NA*) in the dataset and added levels for response as "good" and "bad".

```{r}
load("data_17_Feb.RData")
data=data[,2:56]

# delete 2557 missing values in "compt_size"
sum(is.na(data$compt_size))
data=na.omit(data)

dim(data)
sum(is.na(data)) #check missing value

#make levels for response
levels(data$class)=c("good","bad")
str(data)
prop.table(table(data$class)) #proportions of our outcome variable
```

Our data isn't perfectly balanced but it certainly isn't skewed or considered a rare event.

I added levels for response, because when you ask for class probabilities, model predictions are a data frame with separate columns for each class/level. If *class* doesn't have levels, `data.frame` converts them to valid names, which creates a problem because a different (but valid) name of levels.


## Data splitting for training and testing

```{r,echo=FALSE}
set.seed(100)
#Spliting data as training and test set. 75% percentage of data in the training set.
Train <- createDataPartition(y =data$class,p = 0.75,list = FALSE)  
train <- data[Train,]
test <- data[-Train,]
dim(train)
dim(test)

```



```{r,echo=FALSE,eval=FALSE}
#Ignore this subset
set.seed(10)
inTrain<-createDataPartition(y =data$class,p = 0.1,list = FALSE)  
Data<-data[inTrain,]
dim(Data)

set.seed(100)
#Spliting data as training and test set. 90% percentage of data in the training set.
Train <- createDataPartition(y =Data$class,p = 0.75,list = FALSE)  
train <- Data[Train,]
test <- Data[-Train,]
dim(train)
dim(test)
```

# Model tuning

```{r,eval=FALSE}
#Setup CV Folds
#returnData=FALSE saves some space
folds=10
repeats=3
# Try using ROC (AUC) as metric since that is what caretEnsemble uses
#trainMetric = "ROC" # Classification, required for some two class analyses

myControl <- trainControl(method='cv', number=folds, repeats=repeats, 
                          returnResamp='none', classProbs=TRUE,
                          returnData=FALSE, savePredictions=TRUE, 
                          verboseIter=TRUE, allowParallel=TRUE,
                          summaryFunction=twoClassSummary,
                          # For caretEnsemble, make the resampling indexes all the same.
                          index=createMultiFolds(train$LOAN_CLASS, k=folds, times=repeats)
                          )
PP <- c('center', 'scale')


```




```{r,eval=FALSE}
set.seed(800)
detectCores()
registerDoParallel(48,cores=48)
getDoParWorkers()

model_list<- caretList(
  LOAN_CLASS~., data=train,
  trControl=myControl,
  methodList=c('blackboost', 'parRF'),
  tuneList=list(
    #gbm=caretModelSpec(method='gbm',
                       #tuneGrid=expand.grid(.n.trees=c(1:10)*10, .interaction.depth=1, .shrinkage = 0.1)
                       #),
    mlpweightdecay=caretModelSpec(method='mlpWeightDecay', trace=FALSE, preProcess=PP),
    earth=caretModelSpec(method='earth', preProcess=PP),
    glm=caretModelSpec(method='glm', preProcess=PP),
    svmRadial=caretModelSpec(method='svmRadial', preProcess=PP),
    knn=caretModelSpec(method='knn',preProcess=PP),
    #gam=caretModelSpec(method='gam', preProcess=PP),
    glmnet=caretModelSpec(method='glmnet', preProcess=PP)
    )
  )

```

```{r,eval=FALSE,echo=FALSE}
#Ignore these individual models
save(file="modellist",list="model_list")
model1 <- train(class~., data=train, method='gbm', trControl=myControl,
                tuneGrid=expand.grid(.n.trees=c(1:20)*10, .interaction.depth=c(1:5), .shrinkage = 0.1))
model2 <- train(class~., data=train, method='blackboost', trControl=myControl)
model3 <- train(class~., data=train, method='parRF', trControl=myControl)
model4 <- train(class~., data=train, method='mlpWeightDecay', trControl=myControl, trace=FALSE, preProcess=PP)
model6 <- train(class~., data=train, method='earth', trControl=myControl, preProcess=PP)
model7 <- train(class~., data=train, method='glm', trControl=myControl, preProcess=PP)
model8 <- train(class~., data=train, method='svmRadial', trControl=myControl, preProcess=PP)
model9 <- train(class~., data=train, method='gam', trControl=myControl, preProcess=PP)
model10 <- train(class~., data=train, method='glmnet', trControl=myControl, preProcess=PP)
```

#Benchmark models (candidate)


We considered 9 candidate models. Some are the least interpretable and most flexible, such as *boosted trees*, *Stochastic Gradient Boosting* or *support vector machines* which have a high likelihood of producing the most accurate results. Others are simpler models that are less opaque (e.g., not complete black boxes), such as *multivariate adaptive regression splines (MARS)* or *generalized additive models*. The model and argument value are in `names(model_list)` and `names(infor)`, respectively. The more detail method information such as tuning parameters can be explored on [caret model list](http://topepo.github.io/caret/modelList.html).



```{r,message=FALSE,warning=FALSE}
load("modellist3") #"model_list" is saved in "modellist3"

#Make a list of all the models
names(model_list) = sapply(model_list, function(x) x$method) #method names
infor = sapply(model_list, function(x) x$modelInfo$label) #label of methods
names(infor) = infor 
names(model_list)
names(infor)
sort(sapply(model_list, function(x) max(x$results$ROC))) #maximum accuracy of each method

```

#Make a greedy ensemble


```{r,message=FALSE,warning=FALSE}
#Make a greedy ensemble - currently can only use RMSE
greedy <- caretEnsemble(model_list, iter=1000L)
sort(greedy$weights, decreasing=TRUE)
#summary(greedy)
greedy$error
```

The greedy model relies 87\% on the **blackboost**, **glm** and **earth**, which makes sense as these are top three highest accuracte models on the training set. The ensembleâ€™s AUC (area under the curve) on the training set is 0.87, which is about 1\% better than the best individual model (**blackboost**).

#Make a linear regression ensemble

```{r,warning=FALSE,message=FALSE}
#Make a linear regression ensemble
linear <- caretStack(model_list, method='glm', trControl=trainControl(method='cv'))
summary(linear$ens_model$finalModel)
linear$error
CF <- coef(linear$ens_model$finalModel)[-1] #coefficient of glm model
CF/sum(CF) #glm model weights
```

The linear model uses all of the models, and achieves an AUC of 0.81, which lower than the average individual model accuracy. I'm not sure if this is a failure of the stacking model, because accuracy becomes much higher later on in the test set prediction. 

Different from greedy model, The glm-weighted model weights relies most on only **blackboost**, and the weight is 0.87.


#Result (predict for test set)


```{r,eval=FALSE}
library(caTools)
model_preds <- lapply(model_list, predict, newdata=test, type='prob')
model_pred <- lapply(model_preds, function(x) x[,'good'])
model_pred <- data.frame(model_pred)
ENS_greedy <- predict(greedy, newdata=test)
model_pred$ENS_greedy=1-ENS_greedy
ENS_linear= predict(linear, newdata=test,type='prob')
model_pred$ENS_linear <- ENS_linear[,"good"]
```

model_pred$truth=test$class
allgood=model_pred[apply(model_pred[,c(1:9)]>=0.7,1,all) , ]  # patients only have triage variables
allbad=model_pred[apply(model_pred[,c(1:9)]<0.5,1,all) , ]  # patients only have triage variables
dim(allgood)
dim(allbad)
dim(allgood)[1]+dim(allbad)[1]

table(allgood$truth)
table(allbad$truth)

allgood=model_preds[apply(model_preds[,c(1:)]=="good",1,all) , ]  # patients only have triage variables
allbad=model_preds[apply(model_preds[,c(1:3)]=="bad",1,all) , ]  # patients only have triage variables
dim(allgood)
dim(allbad)

allgood

model_pred$gam=1-model_pred$gam

pred=rep("bad",3429)
pred[model_pred$gbm>.6]="good"
pred=factor(pred)
levels(pred)=c("bad","good")
table(pred,test$class)
con=confusionMatrix(pred, test$class)
confusionMatrix(predict(svm.tune1, test[,-1]), test$Diagnosis)

sum(model_pred$gbm>=.5)

```{r}
library(caTools)
load("pred")  #"model_pred" is saved in "pred"
sort(data.frame(colAUC(model_pred, test$class)))
```

The greedy model testing prediction still better than any individual model prediction, and has the highest accuracy 0.87.

Most predictions in test set are reasonable, except "linear regression ensemble", I mentioned before, that it has much higher testing accuracy 0.867 than training accuracy 0.81. *I've no idea why it happened*.